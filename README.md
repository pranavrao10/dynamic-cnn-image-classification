# Dynamic CNN for CIFAR-10 Image Classification

This project implements a Dynamic Convolutional Neural Network (CNN) architecture for image classification on the CIFAR-10 dataset. The architecture dynamically combines multiple convolutional layers within intermediate blocks to improve classification accuracy.

## Project Structure

- `dynamic-cnn-image-classification.ipynb`: Main Jupyter Notebook containing the full implementation, training loop, and results.
- `training_metrics.png`: Contains visualizations of training loss and accuracy over epochs.

## Key Features

- Dynamic Intermediate Blocks: Modular convolutional blocks dynamically weighting layer outputs.
- Adaptive Feature Weighting: Uses fully connected layers to learn optimal weights for feature integration.
- Comprehensive Training Pipeline: Implemented training, evaluation, and hyperparameter tuning pipeline.
- Performance Monitoring: Visual tracking of loss, training accuracy, and testing accuracy over epochs.

## How It Works

### Pipeline

1. Data Preparation and Augmentation
   - Data Loading: CIFAR-10 images are loaded using PyTorch DataLoaders.
   - Transformations: Includes random cropping, horizontal flipping, rotations, color jittering, and normalization to enhance model robustness.
2. Dynamic CNN Architecture
   - Intermediate Blocks:
     - Each block comprises multiple parallel convolutional layers.
     - Features from these layers are combined dynamically using weights generated by fully connected layers based on channel wise averages.
     - Layers include Batch Normalization and ReLU activations with Kaiming initialization.
   - Output Block:
     - Aggregates final features through channel wise averaging.
     - Passes the aggregated features through fully connected layers with Batch Normalization, ReLU activations, and Dropout layers for robust classification.
3. Training and Optimization
   - Optimizer: Uses SGD with a learning rate of 0.15, momentum of 0.95, Nesterov acceleration, and weight decay.
   - Learning Rate Scheduler: Implements a One Cycle LR scheduler for adaptive learning rates.
   - Mixed Precision Training: Utilizes PyTorch’s GradScaler for efficient mixed precision training, boosting computational efficiency.
   - Loss Function: Cross-Entropy Loss with label smoothing to enhance generalization.
4. Evaluation and Monitoring
   - Metrics: Tracks accuracy on both training and testing datasets after each epoch.
   - Visualizations: Training loss, training accuracy, and testing accuracy plotted to provide clear insights into model performance over epochs.

## Setup and Installation

### Here is how to run the project locally:

1. Clone the repository:
   - git clone https://github.com/pranavrao10/dynamic-cnn-image-classification.git
   - cd dynamic-cnn-image-classification
2. Install dependencies:
   - python -m venv env
   - source env/bin/activate
   - pip install -r requirements.txt
3. Run the notebook:
   - Open dynamic-cnn-image-classification.ipynb on Google Colab or locally with Jupyter Notebook.
   - Run notebook cells sequentially.

## Data

- Dataset: CIFAR-10
- Number of Images: 60,000 (50,000 train, 10,000 test)
- Classes: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck
- Image Dimensions: RGB, 32x32 pixels

## Performance

- Highest Test Accuracy: 92.15%
- Training Accuracy Progression: Clearly visualized improvements over epochs.
- Batch-wise Loss Tracking: Detailed insights into training dynamics.

## Insights

- Key components contributing to success:
  - Dynamic weighting of convolutional layers significantly enhanced the model’s ability to capture complex image features.
  - Aggressive data augmentation (random cropping, flipping, rotations, and color jittering) substantially improved model generalization.
  - Advanced optimization techniques: - Mixed-precision training with PyTorch’s GradScaler for efficiency and stable convergence. - One Cycle Learning Rate scheduling and carefully tuned hyperparameters (learning rate, momentum, weight decay) significantly accelerated training and boosted accuracy.
  - Limitations: - Despite high accuracy, there’s still potential improvement in generalization to more challenging or noisy real world datasets. - Dynamic architectures, while powerful, require more computational resources and careful tuning.
  - Future Improvements: - Exploring deeper or wider dynamic blocks to improve feature representation. - Integrating attention mechanisms to further refine dynamic weighting strategies. - Applying transfer learning from larger datasets (e.g., ImageNet) to boost generalization capability.

## Contributors:

- Pranav Rao (@pranavrao10)
